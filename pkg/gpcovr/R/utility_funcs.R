#' Bayesian spline fitting
#'
#' @param B the number of MCMC iterations
#' @param y a vector of observations
#' @param spl a natural cubic spline basis matrix, such as one generated by
#'   \code{\link{nsbasis}}
#' @param burnin the number of iterations to discard as burn-in
#' @param init a named list of initial guesses for \code{beta} (the basis
#'   coefficients), \code{sigma} (the model error estimate), and \code{tau} (the
#'   smoothness penalty).
#' @param tune_init a named list of initial guesses for tuning parameters
#' @param hyperparams a named list of hyperparameters for the priors on
#'   \code{sigma} and \code{tau}. Currently these priors are both hard coded as
#'   inverse gamma.
#' @param c0 adaptive tuning parameter
#' @param c1 adaptive tuning parameter
#' @param k adaptive tuning parameter
#' @param r lag for adaptive tuning
#' @param opt.acc.rate optimal accept rate for adaptive tuning
#' @param progress not currently used
#' @return A list of length 3. The first element is a matrix where each row is a
#'   random \code{beta} draw. The second and third are vectors where each
#'   element is a random draw of \code{sigma} and \code{tau} respectively.
#' @export
fitspline <- function(B, y, spl,
                      burnin = floor(B / 5),
                      init = list(beta = rep(1, ncol(spl)), sigma = 1, tau = 1),
                      tune_init = list(beta = rep(1, ncol(spl)), sigma = 1, tau = 1),
                      hyperparams = list(sigma = c(1, 0.005), tau = c(1, 0.005)),
                      c0 = 10, c1 = 0.8, k = 3, r = 5, opt.acc.rate = 0.41, progress = T)
{
  n <- ncol(spl)

  # storage
  beta_samp <- matrix(NA, nrow = B, ncol = n)
  sigma_samp <- rep(NA, B)
  tau_samp <- rep(NA, B)

  beta_accepts <- matrix(0, nrow = B, ncol = n)
  sigma_accepts <- rep(0, B)
  tau_accepts <- rep(0, B)

  v_beta <- matrix(NA, nrow = B, ncol = n)
  v_sigma <- rep(NA, B)
  v_tau <- rep(NA, B)

  # starting values
  beta <- init$beta
  sigma <- init$sigma
  tau <- init$tau

  beta_samp[1, ] <- beta
  sigma_samp[1] <- sigma
  tau_samp[1] <- tau

  beta_accepts[1, ] <- rep(1, n)
  sigma_accepts[1] <- 1
  tau_accepts[1] <- 1

  v_beta[1, ] <- tune_init$beta
  v_sigma[1] <- tune_init$sigma
  v_tau[1] <- tune_init$tau

  s_a <- hyperparams$sigma[1]
  s_b <- hyperparams$sigma[2]
  t_a <- hyperparams$tau[1]
  t_b <- hyperparams$tau[2]

  f <- fitsplinecpp(B, y, spl, burnin, beta, sigma, tau, v_beta[1, ], v_sigma[1], v_tau[1], s_a, s_b, t_a, t_b, c0, c1, k, r, opt.acc.rate, FALSE)

  return(list(b = f$b,
              s = drop(f$s),
              t = drop(f$t)))
}


#' Matern spectral density function
#'
#' @param x A value
#' @param nu Smoothness parameter
#' @param alpha Inverse range parameter
#' @param sigma Spread parameter
#' @param d The dimension of the Gaussian process
#' @return The Matern density evaluated at \code{x}: \deqn{\frac{\sigma \Gamma
#'   \left(\nu + \frac{d}{2} \right) (4\nu)^{\nu}}{\pi^{d/2}\rho^{2\nu}
#'   \Gamma(\nu)} \left(\frac{4\nu}{\rho^2} + x^2 \right)^{-(\nu + d/2)}}
#' @export
dmatern <- function(x, nu, alpha, sigma, d = 2)
{
  exp(log(sigma) + lgamma(nu + d/2) + nu * log(4 * nu) - d/2 * log(pi) - lgamma(nu) + 2 * nu * log(alpha) - (nu + d/2) * log(4 * alpha * alpha * nu + x * x))
}


#' rmatern
#'
#' Draws random samples from a Matern spectral density.
#' @param n the number of samples to draw
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @param df degrees of freedom of the envelope t density. This can be tuned to get a better acceptance rate
#' @return a vector of random draws from the Matern spectral density
#' @export
#' @export
rmatern <- function(n, nu, alpha, sigma, df)
{
  samps <- NULL
  while(length(samps) < n)
  {
    t <- stats::rt(5*n, df = df)
    scale <- find_max_ratio(nu, alpha, sigma, df)
    ratio <- dmatern(t, nu, alpha, sigma) / (1.01 * scale * stats::dt(t, df = df))
    accept <- stats::runif(5*n) < ratio
    samps <- c(samps, t[accept])
  }
  samps <- samps[1:n]
  return(samps)
}


#' Ratio of Matern density to t density
#'
#' @param x a value
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @param df degrees of freedom of the t density
#' @return The ratio of the Matern density to the t density, evaluated at \code{x}
#' @export
rat_matern_t <- function(x, nu, alpha, sigma, df)
{
  dmatern(x, nu, alpha, sigma) / stats::dt(x, df)
}


#' Find max ratio
#'
#' Find the maximum of \code{rat_matern_t}, the ratio of the Matern density to the t density.
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @param df degrees of freedom of the t distribution
#' @return the maximum value of f(x), where f is the ratio of the Matern to the t
#' @seealso \code{\link{rat_matern_t}}
#' @export
find_max_ratio <- function(nu, alpha, sigma, df)
{
  xmax <- 1000 * alpha
  stats::optimize(rat_matern_t, interval = c(0, xmax), maximum = T, nu = nu, alpha = alpha, sigma = sigma, df = df)$objective
}



#' Matern covariance function
#'
#' The true Matern covariance function.
#' @param h distance value/vector/matrix
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @return The covariance associated with the distance h
#' @export
matern_cor <- function(h, nu, alpha, sigma) {
  out <- rep(sigma, length(h))
  idx_non0 <- which(h != 0)
  tmp <- 2 * sqrt(nu) * h[idx_non0] * alpha
  out[idx_non0] <- sigma * tmp^nu * besselK(tmp, nu) / (2^(nu-1) * gamma(nu))
  if(is.matrix(h)) dim(out) <- dim(h)
  out
}




#' Generate a Gaussian process
#'
#' Generates a Gaussian random field on the unit square. Observation locations
#' are guaranteed to be a certain minimum distance apart.
#' @param n the number of observations to generate
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @param tau nugget effect
#' @param mindist the minimum distance between two observation locations
#' @return a list with two elements. The first is a data frame with the x and y
#'   coordinates of the observation locations and z, which is the value of the
#'   observation. The second is a matrix of distances between the observations.
#'   We return this so that it doesn't need to be calculated separately later
#'   on.
#' @export
generateGP <- function(n, nu, alpha, sigma, tau, mindist = 0.005) {
  # grid coordinates from 0.005 to 0.995 in increments of 0.015
  x <- seq(mindist, 1 - mindist, by = 3 * mindist)
  if(n > length(x)^2)
  {
    stop(paste0('when mindist = ', mindist, ', sample size cannot exceed ', length(x)^2, '.'))
  }
  # create grid with small perturbation at each point
  grid_all <- expand.grid(x = x, y = x) + stats::runif(2*length(x)^2, -mindist, mindist)
  # randomly sample n points within this grid
  grid <- grid_all[sample(1:nrow(grid_all), size = n), ]
  # calculate distances between every pair of points
  distances <- as.matrix(stats::dist(grid))
  # calculate Matern covariance function at each of those distances
  covmat <- matern_cor(distances, nu, alpha, sigma)
  # diagonal of sigma is 0, change it to 1 + tau2
  diag(covmat) <- sigma + tau
  ## generate rv's with covariance matrix = sigma
  R <- chol(covmat)
  u <- stats::rnorm(nrow(grid))
  z <- crossprod(R, u)

  return(list(dat = data.frame(grid, z = z),
              dist = distances))
}


#' Approximated normal log likelihood, Matern covariance
#'
#' @param x vector of observations
#' @param h distance matrix between the observations
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @param tau nugget effect
#' @param num_samps the number of MC samples to use. Ignored if \code{mc} is not null.
#' @param mc_samps vector of MC samples. If \code{NULL}, generate them with calls to \code{\link{rmatern}} and \code{gpumc::mc}.
#' @return the log likelihood.
#' @export
normal_ll <- function(x, h, nu, alpha, sigma, tau, num_samps, mc_samps = NULL)
{
  n <- length(x)
  covmat <- diag(1+tau, nrow = n, ncol = n)
  if(is.null(mc_samps))
  {
    samps <- rmatern(num_samps, nu, alpha, sigma, 0.1)
    mc_samps <- gpumc::mc(h[lower.tri(h)], samps)
  }
  covmat[lower.tri(covmat)] <- mc_samps
  covmat[upper.tri(covmat)] <- t(covmat)[upper.tri(covmat)]

  cholmat <- chol(covmat)
  log_det <- sum(log(diag(cholmat)))
  quadform <- drop(crossprod(backsolve(cholmat, x, transpose = TRUE)))
  ll <- -0.5 * (n * log(2*pi) + 2 * log_det + quadform)

  ll
}



#' Exact normal log likelihood
#'
#' @param x vector of observations
#' @param h distance matrix between the observations
#' @param nu smoothness parameter
#' @param alpha inverse range parameter
#' @param sigma scale parameter
#' @param tau nugget effect
#' @return the log likelihood.
#' @export
normal_ll_exact <- function(x, h, nu, alpha, sigma, tau)
{
  covmat <- matern_cor(h, nu, alpha, sigma)
  diag(covmat) <- sigma + tau
  cholmat <- chol(covmat)
  log_det <- sum(log(diag(cholmat)))
  quadform <- drop(crossprod(backsolve(cholmat, x, transpose = TRUE)))
  -0.5 * (length(x) * log(2*pi) + 2 * log_det + quadform)
}


#' Calculate the error bounds for the log-log spectral density
#'
#' @param b A matrix in which each row is a vector of spline basis coefficients
#' from the MCMC
#' @param x A vector of x values to evaluate the error bounds over
#' @param knots The knots used in the MCMC
#' @return The 2.5% and 97.5% percentiles at each x value
#' @export
get_error_bounds <- function(b, x, knots)
{
  curves <- apply(b, 1, function(beta) dloglogspline(x, beta, knots))
  quantiles <- t(apply(curves, 1, function(x) stats::quantile(x, c(0.025, 0.975))))
  quantiles
}

#' Calculate the error bounds for the covariance function
#'
#' WARNING: this uses the GPU Monte Carlo integration and will crash your
#' computer if you do more than 1000 - 5000 coefficient vectors at a time.
#' This is because R is stupid and doesn't free up memory when it needs to.
#'
#' TODO: make this take every 100th or so row from b and estimate the
#' bounds that way.
#'
#' @param b A matrix in which each row is a vector of spline basis coefficients
#' from the MCMC
#' @param x A vector of x values to evaluate the error bounds over
#' @param knots The knots used in the MCMC
#' @return The 2.5% and 97.5% percentiles at each x value
#' @export
get_cov_error_bounds <- function(b, x, knots)
{
  curves <- apply(b, 1, function(beta) gpumc::mc(x, exp(rlogspline(50000, beta, knots))))
  quantiles <- t(apply(curves, 1, function(x) stats::quantile(x, c(0.025, 0.975))))
  quantiles
}


#' Create observation and prediction locations for simulating a Gaussian process
#'
#' Generates a \code{GPlocations} object, containing locations for both
#' (non-gridded) observations and (gridded) predictions when the Gaussian
#' process is simulated
#'
#' @param M The number of observation locations
#' @param ngrid The prediction grid will be \code{ngrid} x \code{ngrid} over the
#'   unit square.
#' @param mindist The minimum possible distance between any two observations. If
#'   two observations are too close together, it may cause numerical
#'   instability.
#'
#' @return A \code{GPlocations} object containing the following:
#'   \itemize{
#'     \item \code{locs}: a data frame with three columns - \code{x} and \code{y}
#'     are the coordinates of the locations, and \code{type} is a factor equal to
#'     \code{obs} for an observation location and \code{pred} for a prediction
#'     location.
#'     \item \code{dist_obs}: an \code{M} x \code{M} matrix consisting of the
#'     distances between the observation locations
#'     \item \code{dist_pred}: an \code{ngrid} x \code{ngrid} matrix consisting
#'     of the distances between the prediction locations
#'     \item \code{mindist}: The \code{mindist} parameter for reference
#'   }
#' @export
#'
#' @examples
#' locations <- create_locations(50, 20, mindist = 0.002)
create_locations <- function(M, ngrid, mindist = 0.005) {
  # create grid of prediction observations
  xpred <- seq(0, 1, length = ngrid)
  grid_pred <- expand.grid(x = xpred, y = xpred)
  # create initial grid for non-gridded observations over unit square.
  # points 3*mindist apart.
  xobs <- seq(mindist, 1-mindist, by = 3*mindist)
  grid_obs <- expand.grid(x = xobs, y = xobs)
  # perturb grid points by no more than mindist in any direction
  grid_obs <- grid_obs + stats::runif(2 * length(xobs) * length(xobs), -mindist, mindist)
  # sample M of these points
  grid_obs <- grid_obs[sample(nrow(grid_obs), size = M), ]
  # concatenate these points and indicate whether they are prediction or
  # observation locations
  grid <- rbind(grid_obs, grid_pred)
  h <- as.matrix(stats::dist(grid))
  dimnames(h) <- NULL
  grid$type <- factor(c(rep('obs', M), rep('pred', ngrid * ngrid)))
  # split the distance matrix into prediction/observation locations only
  h_obs <- h[grid$type == 'obs',grid$type == 'obs']
  h_pred <- h[grid$type == 'pred',grid$type == 'pred']
  # create output
  out <- list(locs = grid, dist_obs = h_obs, dist_pred = h_pred, mindist = mindist)
  class(out) <- 'GPlocations'
  return(out)
}


#' Prepare the covariance model
#'
#' Creates the model (from the RandomFields package) and spectral density
#' function for the specified covariance family.
#'
#' This gets called internally by \code{\link{simulate_gp}}.
#'
#' @param family The parametric family the covariance function will belong to
#' @param params A vector of parameters for the covariance function. They must be
#' \itemize{
#'   \item \code{matern}: (nu, rho, sigma, nugget)
#'   \item \code{dampedcos}: (lambda, theta, sigma, nugget)
#' }
#'
#' @return A \code{GPmodel} object containing the following:
#' \itemize{
#'   \item \code{model}: The model, from the \code{RandomFields} package
#'   \item \code{params}: The parameter vector
#'   \item \code{specdens}: The spectral density function
#' }
#' @export
#'
#' @examples
#' model <- prepare_model('matern', c(1.5, 0.2, 1, 0.01))
prepare_model <- function(family, params) {
  if (family == 'matern') {
    names(params) <- c('nu', 'rho', 'sigma', 'nugget')
    model <- RandomFields::RMhandcock(params['nu'], notinvnu = TRUE, scale = params['rho'], var = params['sigma']) +
      RandomFields::RMnugget(var = params['nugget'])
    specdens <- function(w) dmatern(w, nu = params['nu'], alpha = 1/params['rho'], sigma = params['sigma'])
  } else if (family == 'dampedcos') {
    names(params) <- c('lambda', 'theta', 'sigma', 'nugget')
    model <- RandomFields::RMdampedcos(lambda = params['lambda'], scale = params['theta'], var = params['sigma']) +
      RandomFields::RMnugget(var = params['nugget'])
    integrand <- function(h, w) {
      besselJ(h*w, 0) * h * RandomFields::RFcov(model, h)
    }
    specdens <- Vectorize(function(w) {
      int <- stats::integrate(integrand, w = w, 0, Inf)$value
      1/(2*pi) * int
    }, vectorize.args = 'w')
  } else {
    stop('Family must be one of (matern, dampedcos)')
  }

  out <- list(model = model, params = params, specdens = specdens)
  class(out) <- 'GPmodel'
  return(out)
}


#' Simulate a Gaussian process
#'
#' @param locations A \code{GPlocations} object
#' @param family The parametric family of the covariance function
#' @param params The parameters for the covariance function
#'
#' @return A \code{GPsimulated} object. This consists of the locations object,
#'   with additional subobjects: \itemize{ \item \code{m}: The \code{GPmodel}
#'   object created by \code{\link{prepare_model}} \item \code{Y}: A vector of
#'   observations. These correspond to ALL locations, observed and predicted.
#'   Subset these according to \code{locations$locs$type}. }
#' @export
#'
#' @examples
#' locations <- create_locations(50, 20, mindist = 0.002)
#' gp <- simulate_gp(locations, 'matern', c(1.5, 0.2, 1, 0.01))
simulate_gp <- function(locations, family, params) {
  RandomFields::RFoptions(spConform = FALSE)
  locations$m <- prepare_model(family, params)
  locations$Y <- as.vector(RandomFields::RFsimulate(locations$m$model, locations$locs$x, locations$locs$y))
  class(locations) <- 'GPsimulated'
  return(locations)
}


#' Plot a GPlocations object
#'
#' Plots both the prediction and observation locations.
#'
#' @param x A \code{GPlocations} object
#' @param ... Additional parameters to \code{\link[graphics]{plot}}
#'
#'
#' @export
#'
#' @examples
#' locations <- create_locations(50, 20, mindist = 0.002)
#' plot(locations)
plot.GPlocations <- function(x, ...) {
  pred_idx <- which(x$locs$type == 'pred')
  grid_pred <- x$locs[pred_idx, ]
  grid_obs <- x$locs[-pred_idx, ]
  graphics::plot.default(grid_pred$x, grid_pred$y, col = 'grey', ...)
  graphics::points(grid_obs$x, grid_obs$y, pch = 19)
}

#' Plot a GPsimulated object
#'
#' Plots the gridded prediction values using \code{\link[graphics]{image}}, and
#' draws the observation values on top with points.
#'
#' @param x A \code{GPsimulated} object
#' @param ... Not used
#' @param bw \code{TRUE} if the plot should be generated in black and white. Defaults to \code{FALSE}.
#'
#' @export
#'
#' @examples
#' locations <- create_locations(50, 20, mindist = 0.002)
#' gp <- simulate_gp(locations, 'matern', c(1.5, 0.2, 1, 0.01))
#' plot(gp)
plot.GPsimulated <- function(x, ..., bw = FALSE) {
  pred_idx <- which(x$locs$type == 'pred')
  grid_pred <- x$locs[pred_idx, ]
  grid_obs <- x$locs[-pred_idx, ]
  Y_pred <- x$Y[pred_idx]
  Y_obs <- x$Y[-pred_idx]
  pal <- ifelse(bw, 'Greys', 'Blues')
  graphics::image(unique(grid_pred$x),
        unique(grid_pred$y),
        matrix(Y_pred, nrow = length(unique(grid_pred$x)), byrow = T),
        col = rev(RColorBrewer::brewer.pal(7, pal)),
        xlab = '', ylab = '')
  graphics::points(grid_obs$x, grid_obs$y, col = 'white', pch = 20)
  graphics::points(grid_obs$x, grid_obs$y, col = 'black', pch = 1)
}


#' Calculate the approximate likelihood from the spline coefficients
#'
#' @param beta The spline coefficients. Vector of length \code{k}
#' @param knots The knot locations. Vector of length \code{k}
#' @param Y The observations. Vector of length \code{M}
#' @param H The distance matrix between the observations. \code{M} x \code{M}
#'   matrix
#' @param B The number of random draws to take from the spline-approximated
#'   spectral density
#' @param nugget The nugget effect
#' @param debug \code{TRUE} to print out debugging information. (This is
#'   untested inside the \code{gpcovr} package. Best to leave this
#'   \code{FALSE}.)
#'
#' @return The approximate log likelihood
#' @export
likelihood <- function(beta, knots, Y, H, B, nugget, debug) {
  M <- length(Y)

  slopes <- get_slopes(beta, knots)

  if(debug) cat('slopes:', slopes, '\n')
  if(slopes[2] >= 0 | slopes[1] <= 0) return(-Inf)
  mineigen <- -Inf
  cnt <- 0
  while(mineigen < 0) {
    cnt <- cnt + 1
    if(cnt != 1) cat('Redrawing...\n')
    if(cnt > 2) {
      cat('Likelihood calculation failed. Returning -Inf\n')
      return(-Inf)
    }
    # take random samples from spectral density
    wtilde <- rlogspline(B, beta, knots)
    if(any(is.infinite(exp(wtilde)))) return(-Inf)
    if(debug) {
      ### plot current density estimate
      # dev.off()
      graphics::par(mfrow = c(2,1))
      x <- seq(-20, 5, length = 200)
      graphics::plot(x, predict_natspl(nsbasis(x, knots), beta), type = 'l', ylim = c(-15, 5))
      graphics::abline(v = knots, col = 'grey', lty = 2)
      graphics::hist(wtilde, probability = T, breaks = 50)
      graphics::lines(x, dlogspline(x, beta, knots))
    }
    # estimate covariance matrix via MC integration
    Sigma <- diag(1 + nugget, nrow = M, ncol = M)
    Sigma[lower.tri(Sigma)] <- gpumc::mc(H[lower.tri(H)], exp(wtilde))
    Sigma[upper.tri(Sigma)] <- t(Sigma)[upper.tri(Sigma)]
    mineigen <- min(eigen(Sigma, symmetric = TRUE, only.values = TRUE)$value)
    if(debug) cat('min eigval is', mineigen, '\n')
    rm(wtilde)
  }
  cholmat <- chol(Sigma)
  logdet <- sum(log(diag(cholmat)))
  quadform <- drop(crossprod(backsolve(cholmat, Y, transpose = TRUE)))
  l <- -0.5 * (M * log(2*pi) + 2*logdet + quadform)
  rm(Sigma, cholmat)
  gc()
  return(l)
}
